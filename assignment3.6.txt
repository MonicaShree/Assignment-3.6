1. If 7TB is the available disk space per node (9 disks with 1 TB, 2 disk for operating system etc.
were excluded.). Assuming initial data size is 600 TB. How will you estimate the number of data
nodes (n)?


The formula : 

n = H/d
  = c*r*S/(1-i)*d 
where , H = c*r*S 
and d = (1-i)*d 

 H - hadoop storage .
 d - disk space available per node . 
 c - average compression ratio.
 S - size of data to be moved to hadoop . 
 i - immediate factor .
 r - replication factor . 


solution : 

 H = 600 TB  
 D = 7 TB
 
 n = H/d 
   = 600 / 7 
   = 85.7 
 
Hence , 86 data nodes are needed .   




2.Imagine that you are uploading a file of 500MB into HDFS.100MB of data is successfully
uploaded into HDFS and another client wants to read the uploaded data while the upload is still in
progress. What will happen in such a scenario, will the 100 MB of data that is uploaded will it be
displayed?


 * The default block size in Hadoop 1x is 64 MB and 128 MB in 2x . Lets consider the block size here to be 100 MB . Then we will have 5 blocks replicated thrice . 

 * We have 5 blocks for a file , client , name node and data node . First the client wil take the first block and will approach name node for data node location to store this block . 

 * When the client is aware of datanode information it will reach out to data node and start copying the first block . 
Now , the client will get confirmation on first block and it will initiate the same process for the second block . 

 * So, The reader can read the 100 MB uploaded data while the remaining upload is still in progress .  